{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    Shuyang Fan, Yiran Wang \n",
    "\n",
    "**Student ID(s):**     988301, 987751\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from scipy.stats import mode \n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "def read_data(fileName):\n",
    "    data = pd.read_csv(fileName, header=None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_value(data):\n",
    "    # Make a copy of raw data\n",
    "    copy = data.copy()\n",
    "    # Drop rows with question mark\n",
    "    copy = copy[(copy.astype(str) != '?').all(axis=1)]\n",
    "    copy = copy.dropna()\n",
    "    copy = copy.reset_index(drop=True)\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def equal_width_binning(data, column_indexes, bin_num):\n",
    "    copy = data.copy()\n",
    "    for column in column_indexes:\n",
    "        copy.iloc[:,column] = pd.cut(copy.iloc[:,column], bin_num)\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def equal_frequency_binning(data, column_indexes, bin_num):\n",
    "    copy = data.copy()\n",
    "    for column in column_indexes:\n",
    "        copy.iloc[:,column] = pd.qcut(copy.iloc[:,column], bin_num)\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def kmeans_binning(data, column_indexes, k):\n",
    "    copy = data.copy()\n",
    "    for column in column_indexes:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42).fit(copy.iloc[:,column])\n",
    "        copy.iloc[:,column] = kmeans.labels_\n",
    "    return copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#hold_out the training and test data in the ratio of 8:2    \n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "    X_total = X.shape[0]\n",
    "    assert(X_total == y.size)\n",
    "    np.random.seed(42)\n",
    "    arr_rand = np.random.rand(X.shape[0])\n",
    "    split = arr_rand < np.percentile(arr_rand, test_size*100)\n",
    "    X_test = X[split]\n",
    "    y_test = y[split]\n",
    "    X_train =  X[~split]\n",
    "    y_train = y[~split]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Bayes calculate the product of prior and conditinoals,, take the max\n",
    "#then predict, throw the X-test into the model\n",
    "#then check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_key_with_max_value(dic):\n",
    "    max_value = -1e5\n",
    "    max_key = None\n",
    "    for key in dic:\n",
    "        if dic[key] > max_value:\n",
    "            max_key = key\n",
    "            max_value = dic[key]\n",
    "    return max_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BayesClassifier():\n",
    "\n",
    "    def train(self, X_train, y_train, type_dict):\n",
    "        self.X_train = X_train.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        #extract the possible class labels from training data\n",
    "        self.possible_labels = np.unique(self.y_train)\n",
    "        #get the prior probability of training data\n",
    "        self.prior = self.get_prior(self.y_train)\n",
    "        self.type_dict = type_dict\n",
    "        self.categorical_prob = self.compute_categorical(self.type_dict['nominal'],self.X_train, self.y_train)\n",
    "        self.numeric_prob = self.compute_numeric(self.type_dict['numeric'], self.X_train, self.y_train)\n",
    "    \n",
    "    #function calculates the prior probability for all labels\n",
    "    def get_prior(self, y_train):\n",
    "        train_inputs = y_train\n",
    "        #counts is a list that stores number of each label accordingly\n",
    "        labels, counts = np.unique(train_inputs, return_counts=True)\n",
    "        prior = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            prior[label] = float(counts[i])/len(train_inputs)\n",
    "        return prior      \n",
    "    \n",
    "    #function examines whether an attribute is numeric \n",
    "    def is_numeric_attribute(self, data, column_index):\n",
    "        return is_numeric_dtype(data.iloc[:,column_index])\n",
    "\n",
    "    #function that computes the conditional probability of categorical attribtues\n",
    "    def compute_categorical(self, categorical_indexes, X_train, y_train):\n",
    "        #create a dictionary to store results\n",
    "        categorical_prob = defaultdict(lambda: defaultdict(dict))\n",
    "        # Separate training instances by label\n",
    "        grouped = X_train.groupby(y_train)\n",
    "        \n",
    "        for label in self.possible_labels:\n",
    "            separated = grouped.get_group(label)\n",
    "            #for each attribute in the same class\n",
    "            for column_index in categorical_indexes:\n",
    "                # Only process categorical attribute\n",
    "                #if (self.is_numeric_attribute(X_train, column_index) == False):\n",
    "                # Extract one attribute from group\n",
    "                attribute = separated.iloc[:,column_index]\n",
    "                total_rows = attribute.shape[0]\n",
    "                # Find all possible values of this attribute\n",
    "                possible_values = np.unique(X_train.iloc[:,column_index])\n",
    "                # Call Counter to count the frequency of each value\n",
    "                counts = Counter(attribute)\n",
    "                for value in possible_values:\n",
    "                    if value in counts:\n",
    "                        categorical_prob[column_index][str(value)][label] = counts[value]/total_rows\n",
    "        return categorical_prob\n",
    "    \n",
    "    #function calculated the conditional probability for numeric attribtues\n",
    "    def compute_numeric(self, numeric_indexes, X_train, y_train):\n",
    "        numeric_prob = defaultdict(lambda: defaultdict(dict))\n",
    "        # Sepeate training instances by label\n",
    "        grouped = X_train.groupby(y_train)\n",
    "        \n",
    "        for label in self.possible_labels:\n",
    "            separated = grouped.get_group(label)\n",
    "            for column_index in numeric_indexes:\n",
    "                \n",
    "                # if self.is_numeric_attribute(X_train, column_index):\n",
    "                # Extract the attribute from group\n",
    "                attribute = separated.iloc[:,column_index]\n",
    "                # print(column_index, attribute)\n",
    "                #calculate the mean and standard deviation of each attribute\n",
    "                numeric_prob[column_index]['mean'][label] = attribute.mean()\n",
    "                numeric_prob[column_index]['std'][label] = attribute.std()\n",
    "                # print(label, column_index, numeric_prob[column_index]['std'][label])\n",
    "        # print(numeric_prob)\n",
    "        return numeric_prob\n",
    "    \n",
    "    \n",
    "    def guassian(self, value, mean, stdev):\n",
    "            exponent = math.exp(-((value-mean)**2 / (2 * stdev**2)))\n",
    "            return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponent\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test_copy = X_test.copy()\n",
    "        categorical_prob = self.categorical_prob\n",
    "        numeric_prob = self.numeric_prob \n",
    "        priors = self.prior\n",
    "        possible_labels = self.possible_labels\n",
    "        predicted_outputs = []\n",
    "    \n",
    "        row, column = X_test_copy.shape\n",
    "        for row_index in range(row):\n",
    "            probability = defaultdict(float)\n",
    "            \n",
    "            for label in possible_labels:\n",
    "                probability[label] = safe_log(priors[label])\n",
    "                \n",
    "            for column_index in range(column):\n",
    "                # Get the value of this attribute\n",
    "                value = X_test_copy.iloc[row_index, column_index]\n",
    "                # Get conditional probability\n",
    "                for label in possible_labels:\n",
    "                    #calculate the numeric conditional probabily\n",
    "                    if column_index in self.type_dict['numeric']:\n",
    "                        #print(\"numeric\",column_index)\n",
    "                        try:\n",
    "                            conditional_probability = self.guassian(value, numeric_prob[column_index]['mean'][label], numeric_prob[column_index]['std'][label])\n",
    "                            probability[label] += safe_log(conditional_probability)\n",
    "\n",
    "                        except ZeroDivisionError:\n",
    "                            pass\n",
    "                    elif column_index in self.type_dict['nominal']:\n",
    "                        #print(\"nomial\",column_index)\n",
    "                        if label not in categorical_prob[column_index][str(value)]:\n",
    "                            #assign a tiny probability to each event even if events are unlikely (epsilon smoothing)\n",
    "                            conditional_probability = 1e-9\n",
    "                        else:\n",
    "                            conditional_probability = categorical_prob[column_index][str(value)][label]\n",
    "                        #take the log of each probabilty and sum it\n",
    "                        probability[label] += safe_log(conditional_probability)\n",
    "            # The prediced outcome is the lebel with the highest probability\n",
    "            predicted_outputs.append(find_key_with_max_value(probability))\n",
    "        return predicted_outputs\n",
    "    \n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    return math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def oridinal_to_integer(X, replacement_map):\n",
    "    copy = X.copy()\n",
    "    for column_index in replacement_map:\n",
    "        map = {key:index for index, key in enumerate(replacement_map[column_index])}\n",
    "        copy.iloc[:,column_index] = copy.iloc[:,column_index].map(map)\n",
    "    return copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_predicted, y_truth):\n",
    "    assert(y_predicted.size==y_truth.size)\n",
    "    return np.sum(y_predicted == y_truth)/y_predicted.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_type_dict(type_list, treat_ordinal_as):\n",
    "    type_dict=defaultdict(list)\n",
    "    for index, type in enumerate(type_list):\n",
    "        if (type==0):\n",
    "            type_dict[\"nominal\"].append(index)\n",
    "        elif type==1:\n",
    "            type_dict[treat_ordinal_as].append(index)    \n",
    "        else:\n",
    "            type_dict[\"numeric\"].append(index)\n",
    "    return type_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_baseline(X_train, y_train):\n",
    "    labels = np.unique(y_train)\n",
    "    np.random.seed(42)\n",
    "    y_baseline = np.array([np.random.choice(labels) for i in range(y_train.size)])\n",
    "    return y_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def zero_r_baseline(X_train, y_train):\n",
    "    try:\n",
    "        label = mode(y_train)\n",
    "    except:\n",
    "        print(y_train)\n",
    "    return np.repeat(label.mode, y_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def label_confusion_matrix(y_predicted, y_truth, label):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for index in range(y_predicted.size):\n",
    "        result =  y_predicted[index]\n",
    "        if (result==label):\n",
    "            if ((y_truth[index]) == label):\n",
    "                TP +=1\n",
    "            else:\n",
    "                FP +=1\n",
    "        else:\n",
    "            if ((y_truth[index]) == label):\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "    return TP, TN, FP, FN\n",
    "                \n",
    "  \n",
    "# (TP, TN, FP, FN) = label_confusion_matrix(np.array([1, 0, 0, 1, 2]), np.array([1, 0, 1, 0, 2]), 2)  \n",
    "# print(TP, TN, FP, FN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Try discretising the numeric attributes in these datasets and treating them as discrete variables\n",
    "in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the\n",
    "numeric values into any number of levels (but around 3 to 5 levels would probably be a good\n",
    "starting point). Does discretizing the variables improve classification performance, compared\n",
    "to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 1\n",
    "data_set =\"./datasets/wdbc.data\"\n",
    "data = pd.read_csv(data_set, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {\"train_accuracy\": [], \"test_accuracy\": [], \"gaussian_accuracy\": []}\n",
    "\n",
    "y = data.iloc[:,label_index] \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:,2:],y, test_size=0.33)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "type_list = [2 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "gaussian_result = bayes.predict(X_test)\n",
    "gaussian_accuracy = accuracy(np.array(gaussian_result), np.array(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,50, 2):\n",
    "    discretized_data = equal_width_binning(data, list(range(2,32)), i)\n",
    "    y = discretized_data.iloc[:,label_index]  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(discretized_data.iloc[:,2:],y, test_size=0.33)\n",
    "    bayes = BayesClassifier()\n",
    "    type_list = [0 for i in range(X_train.shape[1])]\n",
    "    bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "\n",
    "    train_result = bayes.predict(X_train)\n",
    "    accuracy_dict[\"train_accuracy\"].append(accuracy(np.array(train_result), np.array(y_train)))\n",
    "    test_result = bayes.predict(X_test)\n",
    "    accuracy_dict[\"test_accuracy\"].append(accuracy(np.array(test_result), np.array(y_test)))    \n",
    "    # Add gaussian accuracy to draw a straight line\n",
    "    accuracy_dict['gaussian_accuracy'].append(gaussian_accuracy)\n",
    "\n",
    "\n",
    "print(len(accuracy_dict[\"test_accuracy\"]))\n",
    "bin_num_df = pd.DataFrame.from_dict(accuracy_dict)\n",
    "bin_num_df.index = list(range(2,50, 2))\n",
    "bin_num_df.columns = ['Train accuracy', 'Test accuracy', \"Gaussian\"]\n",
    "bin_num_df.head()\n",
    "\n",
    "plot = bin_num_df.plot(title='Number of bins vs. Accuracy')\n",
    "plot.set_xlabel(\"Number of bins\")\n",
    "plot.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "data_set =\"./datasets/wine.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "accuracy_dict = {\"train_accuracy\": [], \"test_accuracy\": [], \"gaussian_accuracy\": []}\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "type_list = [2 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "gaussian_result = bayes.predict(X_test)\n",
    "gaussian_accuracy = accuracy(np.array(gaussian_result), np.array(y_test))\n",
    "\n",
    "for i in range(2,50, 2):\n",
    "    accuracy_dict['gaussian_accuracy'].append(gaussian_accuracy)\n",
    "\n",
    "    \n",
    "    discretized_data = equal_width_binning(data, list(range(1,14)), i)\n",
    "    X = discretized_data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "\n",
    "    bayes = BayesClassifier()\n",
    "    type_list = [0 for i in range(X_train.shape[1])]\n",
    "    bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "\n",
    "    train_result = bayes.predict(X_train)\n",
    "    accuracy_dict[\"train_accuracy\"].append((accuracy(np.array(train_result), np.array(y_train))))\n",
    "    \n",
    "    test_result = bayes.predict(X_test)\n",
    "    accuracy_dict[\"test_accuracy\"].append(accuracy(np.array(test_result), np.array(y_test)))    \n",
    "    \n",
    "\n",
    "bin_num_df = pd.DataFrame.from_dict(accuracy_dict)\n",
    "bin_num_df.index = list(range(2,50, 2))\n",
    "bin_num_df.columns = ['Train accuracy', 'Test accuracy', \"Gaussian\"]\n",
    "bin_num_df.head()\n",
    "\n",
    "\n",
    "plot = bin_num_df.plot(title='Number of bins vs. Accuracy')\n",
    "plot.set_xlabel(\"Number of bins\")\n",
    "plot.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve\n",
    "Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance\n",
    "varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dict = {\"bayes\": [], \"random\": [], \"zeroR\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "data_set =\"./datasets/breast-cancer-wisconsin.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "data = handle_missing_value(data)\n",
    "label_index = 10\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "X = X.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "bayes = BayesClassifier()\n",
    "type_list = [0 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"nominal\"))\n",
    "NB_result = bayes.predict(X_test)\n",
    "NB_accuracy = accuracy(np.array(NB_result), np.array(y_test))\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "\n",
    "baseline_dict[\"bayes\"].append(NB_accuracy)\n",
    "baseline_dict[\"random\"].append(accuracy(ran_baseline, y_test))\n",
    "baseline_dict[\"zeroR\"].append(accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 1\n",
    "data_set =\"./datasets/wdbc.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "X = X.iloc[:,2:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = data.iloc[:,label_index] \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:,2:],y, test_size=0.33)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "type_list = [2 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "gaussian_result = bayes.predict(X_test)\n",
    "gaussian_accuracy = accuracy(np.array(gaussian_result), np.array(y_test))\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "baseline_dict[\"bayes\"].append(gaussian_accuracy)\n",
    "baseline_dict[\"random\"].append(accuracy(ran_baseline, y_test))\n",
    "baseline_dict[\"zeroR\"].append(accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_index = 8\n",
    "data_set =\"./datasets/nursery.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "replacement_map = {\n",
    "    0: [\"usual\", \"pretentious\", \"great_pret\"],\n",
    "    1: [\"proper\", \"less_proper\", \"improper\", \"critical\", \"very_crit\"],\n",
    "    2: [\"complete\", \"completed\", \"incomplete\", \"foster\"],\n",
    "    3: [\"1\", \"2\",\"3\", \"more\"],\n",
    "    4: [\"convenient\", \"less_conv\", \"critical\"],\n",
    "    5: [\"convenient\", \"inconv\"],\n",
    "    6: [\"nonprob\", \"slightly_prob\", \"problematic\"],\n",
    "    7: [\"recommended\", \"priority\", \"not_recom\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "last = np.unique(X.iloc[:,-1])\n",
    "last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "#consider all the ordinal attributes as nominal\n",
    "type_list_nominal = [0 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list_nominal, treat_ordinal_as=\"numeric\"))\n",
    "nominal_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as nominal naive bayes result:\",accuracy(np.array(nominal_result), np.array(y_test)))\n",
    "\n",
    "\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "print(\"random baseline result:\", accuracy(ran_baseline, y_test))\n",
    "\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "print(\"zeroR baseline result:\", accuracy(zeror_baseline, y_test))\n",
    "baseline_dict[\"bayes\"].append(accuracy(np.array(nominal_result), np.array(y_test)))\n",
    "baseline_dict[\"random\"].append(accuracy(ran_baseline, y_test))\n",
    "baseline_dict[\"zeroR\"].append(accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 14\n",
    "data_set =\"./datasets/adult.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_set, header=None)\n",
    "data = handle_missing_value(data)\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "type_list = [2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, \"nominal\"))\n",
    "\n",
    "result = bayes.predict(X_test)\n",
    "\n",
    "print(accuracy(np.array(result), np.array(y_test)))\n",
    "\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "print(accuracy(ran_baseline, y_test))\n",
    "\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "print(accuracy(zeror_baseline, y_test))\n",
    "baseline_dict[\"bayes\"].append(accuracy(np.array(result), np.array(y_test)))\n",
    "baseline_dict[\"random\"].append(accuracy(ran_baseline, y_test))\n",
    "baseline_dict[\"zeroR\"].append(accuracy(zeror_baseline, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.DataFrame.from_dict(baseline_dict)\n",
    "\n",
    "baseline_df.index = ['cancer',\"wdbc\",\"nursery\",\"adult\"]\n",
    "\n",
    "\n",
    "plot = baseline_df.plot.bar()\n",
    "\n",
    "plot.set_xlabel(\"Dataset\")\n",
    "plot.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as\n",
    "either nominal variables or numeric variables. Compare these strategies on the ordinal datasets\n",
    "provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_dict = {\"nominal\":[], \"numeric\": [], \"random\":[], \"zeroR\":[]}\n",
    "def ordinal_handling(ordinal_dict, nominal_acc, numeric_acc, random_acc, zeroR_acc):\n",
    "    ordinal_dict[\"nominal\"].append(nominal_acc)\n",
    "    ordinal_dict[\"numeric\"].append(numeric_acc)\n",
    "    ordinal_dict[\"random\"].append(random_acc)\n",
    "    ordinal_dict[\"zeroR\"].append(zeroR_acc)\n",
    "    return ordinal_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 8\n",
    "data_set =\"./datasets/nursery.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "replacement_map = {\n",
    "    0: [\"usual\", \"pretentious\", \"great_pret\"],\n",
    "    1: [\"proper\", \"less_proper\", \"improper\", \"critical\", \"very_crit\"],\n",
    "    2: [\"complete\", \"completed\", \"incomplete\", \"foster\"],\n",
    "    3: [\"1\", \"2\",\"3\", \"more\"],\n",
    "    4: [\"convenient\", \"less_conv\", \"critical\"],\n",
    "    5: [\"convenient\", \"inconv\"],\n",
    "    6: [\"nonprob\", \"slightly_prob\", \"problematic\"],\n",
    "    7: [\"recommended\", \"priority\", \"not_recom\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "#consider all the ordinal attributes as nominal\n",
    "type_list = [1 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"nominal\" ))\n",
    "nominal_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as nominal naive bayes result:\",accuracy(np.array(nominal_result), np.array(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_numeric = oridinal_to_integer(X, replacement_map)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y)\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"numeric\"))\n",
    "numeric_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as numeric naive bayes result:\",accuracy(np.array(numeric_result), np.array(y_test)))\n",
    "\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "print(\"random baseline result:\", accuracy(ran_baseline, y_test))\n",
    "\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "print(\"zeroR baseline result:\", accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_dic = ordinal_handling(ordinal_dict, accuracy(np.array(nominal_result), np.array(y_test)), accuracy(np.array(numeric_result), np.array(y_test)), accuracy(ran_baseline, y_test), accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Somervillie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "data_set =\"./datasets/somerville.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "#consider all the ordinal attributes as nominal\n",
    "type_list = [1 for i in range(X_train.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"nominal\" ))\n",
    "nominal_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as nominal naive bayes result:\",accuracy(np.array(nominal_result), np.array(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"numeric\"))\n",
    "numeric_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as numeric naive bayes result:\",accuracy(np.array(numeric_result), np.array(y_test)))\n",
    "\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "print(\"random baseline result:\", accuracy(ran_baseline, y_test))\n",
    "\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "print(\"zeroR baseline result:\", accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_dic = ordinal_handling(ordinal_dict, accuracy(np.array(nominal_result), np.array(y_test)), accuracy(np.array(numeric_result), np.array(y_test)), accuracy(ran_baseline, y_test), accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "label_index = 6\n",
    "data_set =\"./datasets/car.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "replacement_map = {\n",
    "    0: [\"low\", \"med\", \"high\", \"vhigh\"],\n",
    "    1: [\"low\", \"med\", \"high\", \"vhigh\"],\n",
    "    2: [\"2\", \"3\", \"4\", \"5more\"],\n",
    "    3: [\"2\", \"4\", \"more\"],\n",
    "    4: [\"small\", \"med\", \"big\"],\n",
    "    5: [\"low\", \"med\", \"high\"]\n",
    "}\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "\n",
    "#consider all the ordinal attributes as nominal\n",
    "type_list = [1 for i in range(X.shape[1])]\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"nominal\" ))\n",
    "nominal_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as nominal naive bayes result:\",accuracy(np.array(nominal_result), np.array(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = oridinal_to_integer(X, replacement_map)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y)\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list,treat_ordinal_as=\"numeric\"))\n",
    "numeric_result = bayes.predict(X_test)\n",
    "\n",
    "print(\"treat ordinal as numeric naive bayes result:\",accuracy(np.array(numeric_result), np.array(y_test)))\n",
    "\n",
    "ran_baseline = random_baseline(X_test, np.array(y_test))\n",
    "print(\"random baseline result:\", accuracy(ran_baseline, y_test))\n",
    "\n",
    "zeror_baseline = zero_r_baseline(X_test, np.array(y_test))\n",
    "print(\"zeroR baseline result:\", accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_dic = ordinal_handling(ordinal_dict, accuracy(np.array(nominal_result), np.array(y_test)), accuracy(np.array(numeric_result), np.array(y_test)), accuracy(ran_baseline, y_test), accuracy(zeror_baseline, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_df = pd.DataFrame.from_dict(ordinal_dic)\n",
    "ordinal_df.index = ['nursery',\"somerville\",\"car\"]\n",
    "\n",
    "\n",
    "plot = ordinal_df.plot.bar()\n",
    "\n",
    "plot.set_xlabel(\"Dataset\")\n",
    "plot.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major\n",
    "mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy\n",
    "(you should implement this yourself and do not simply call existing implementations from\n",
    "scikit-learn). How does your estimate of effectiveness change, compared to testing on\n",
    "the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "data_set =\"./datasets/wine.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "accuracy_dict = {\"hold-out\": [], \"all\": []}\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "type_list = [2 for i in range(X_train.shape[1])]\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "hold_out_result = bayes.predict(X_test)\n",
    "accuracy_dict[\"hold-out\"].append(accuracy(np.array(hold_out_result), np.array(y_test)))\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X, y, build_type_dict(type_list, treat_ordinal_as=\"numeric\"))\n",
    "all_result = bayes.predict(X)\n",
    "accuracy_dict[\"all\"].append(accuracy(np.array(all_result), np.array(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 8\n",
    "data_set =\"./datasets/nursery.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "replacement_map = {\n",
    "    0: [\"usual\", \"pretentious\", \"great_pret\"],\n",
    "    1: [\"proper\", \"less_proper\", \"improper\", \"critical\", \"very_crit\"],\n",
    "    2: [\"complete\", \"completed\", \"incomplete\", \"foster\"],\n",
    "    3: [\"1\", \"2\",\"3\", \"more\"],\n",
    "    4: [\"convenient\", \"less_conv\", \"critical\"],\n",
    "    5: [\"convenient\", \"inconv\"],\n",
    "    6: [\"nonprob\", \"slightly_prob\", \"problematic\"],\n",
    "    7: [\"recommended\", \"priority\", \"not_recom\"],\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "type_list_nominal = [0 for i in range(X_train.shape[1])]\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "#consider all the ordinal attributes as nominal\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list_nominal, treat_ordinal_as=\"nominal\"))\n",
    "hold_out_result = bayes.predict(X_test)\n",
    "accuracy_dict[\"hold-out\"].append(accuracy(np.array(hold_out_result), np.array(y_test)))\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "#consider all the ordinal attributes as nominal\n",
    "bayes.train(X, y, build_type_dict(type_list_nominal, treat_ordinal_as=\"nominal\"))\n",
    "all_result = bayes.predict(X)\n",
    "accuracy_dict[\"all\"].append(accuracy(np.array(all_result), np.array(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 14\n",
    "data_set =\"./datasets/adult.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "data = handle_missing_value(data)\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "type_list = [2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "# result = random_baseline(X_test, y_test)\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"nominal\"))\n",
    "hold_out_result = bayes.predict(X_test)\n",
    "accuracy_dict[\"hold-out\"].append(accuracy(np.array(hold_out_result), np.array(y_test)))\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X, y, build_type_dict(type_list, treat_ordinal_as=\"nominal\"))\n",
    "all_result = bayes.predict(X)\n",
    "accuracy_dict[\"all\"].append(accuracy(np.array(all_result), np.array(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set =\"./datasets/mushroom.data\"\n",
    "data = read_data(data_set)\n",
    "\n",
    "data = handle_missing_value(data)\n",
    "\n",
    "label_index = 0\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "type_list = [0 for i in range(X_train.shape[1])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list, treat_ordinal_as=\"nominal\"))\n",
    "hand_out_result = bayes.predict(X_test)\n",
    "accuracy_dict[\"hold-out\"].append(accuracy(np.array(hand_out_result), np.array(y_test)))\n",
    "\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X, y, build_type_dict(type_list, treat_ordinal_as=\"nominal\"))\n",
    "all_result = bayes.predict(X)\n",
    "accuracy_dict[\"all\"].append(accuracy(np.array(all_result), np.array(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 6\n",
    "data_set =\"./datasets/car.data\"\n",
    "data = pd.read_csv(data_set, header=None)\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "type_list_numeric = [2 for i in range(X.shape[1])]\n",
    "\n",
    "replacement_map = {\n",
    "    0: [\"low\", \"med\", \"high\", \"vhigh\"],\n",
    "    1: [\"low\", \"med\", \"high\", \"vhigh\"],\n",
    "    2: [\"2\", \"3\", \"4\", \"5more\"],\n",
    "    3: [\"2\", \"4\", \"more\"],\n",
    "    4: [\"small\", \"med\", \"big\"],\n",
    "    5: [\"low\", \"med\", \"high\"]\n",
    "}\n",
    "\n",
    "y = data.iloc[:,label_index]  \n",
    "X = data.drop(data.columns[label_index], axis=1, inplace=False)\n",
    "\n",
    "X_numeric = oridinal_to_integer(X, replacement_map)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numeric, y)\n",
    "bayes = BayesClassifier()\n",
    "bayes.train(X_train, y_train, build_type_dict(type_list_numeric,treat_ordinal_as=\"numeric\"))\n",
    "hold_out_result = bayes.predict(X_test)\n",
    "accuracy_dict[\"hold-out\"].append(accuracy(np.array(hold_out_result), np.array(y_test)))\n",
    "\n",
    "#consider all the ordinal attributes as numeric\n",
    "bayes.train(X_numeric, y, build_type_dict(type_list_numeric,treat_ordinal_as=\"numeric\"))\n",
    "all_result = bayes.predict(X_numeric)\n",
    "accuracy_dict[\"all\"].append(accuracy(np.array(all_result), np.array(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num_df = pd.DataFrame.from_dict(accuracy_dict)\n",
    "bin_num_df.index = [\"wine\", \"nursery\", \"adult\", \"mushroom\", \"car\"]\n",
    "bin_num_df[\"diff\"] = bin_num_df[\"all\"] - bin_num_df[\"hold-out\"]\n",
    "bin_num_df.columns = [\"hold-out\", \"all\", \"diff\"]\n",
    "bin_num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot = bin_num_df.loc[:,[\"hold-out\", \"all\"]].plot.bar(rot=0)\n",
    "plot.set_ylim([0.7,1])\n",
    "plot.set_xlabel(\"Dataset\")\n",
    "plot.set_ylabel(\"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
